#Data Science Capstone Project
#Merrimack College - January 2021
#Data Cleansing

#Invoke the requried library packages
library(data.table)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(broom)
library(janitor)
library(dlookr)
library(purrr)
library(psych)

#set the working directory to start with in your local machine
setwd ("D:/Merrimack/Capstone Project/R Work - Data Cleansing")

#Read in the Fundamentals data
DSFundamentals <- fread("D:/Merrimack/Capstone Project/Data/Fundamentals_DS.csv", header=TRUE, na.strings = c("", "NA"))
DSFundamentals <- as.data.frame(DSFundamentals)
DSFund <- filter(DSFundamentals, gsector == "25")
DSFund <- subset(DSFund, select = c("gvkey",
                                    "datadate",
                                    "datafmt",
                                    "tic",
                                    "conm",
                                    "ggroup",	#GIC Groups
                                    "gind",	#GIC Industries
                                    "gsector", #GIC Sectors
                                    "gsubind", #GIC Sub-Industries
                                    "sic", #Standard Industry Classification Code
                                    "idbflag", #International, Domestic, Both Indicator
                                    "loc", #Current ISO Country Code - Headquarters
                                    "state", #State/Province
                                    "costat",	#Active/Inactive Status Marker
                                    "acctstd", #Accounting Standard cat
                                    "auop", #Auditor Opinion
                                    "exchg", #Stock Exchange Code
                                    "dlrsn", #Research Co Reason for Deletion
                                    "act", #Current Assets Total cont
                                    "at", #Assets - Total
                                    "capx", #Capital Expenditures cont
                                    "ceq", #Common/Ordinary Equity - Total
                                    "che", #Cash and Short-Term Investments cont
                                    "ci", #Total Comprehensive Income cont
                                    "cshi", #Common Shares Issued
                                    "csho", #Common Shares Outstanding
                                    "dlc", #Debt in Current Liabilities - Total cont
                                    "dltt", #Long-Term Debt - Total cont
                                    "dv", #Cash Dividends (Cash Flow) cont
                                    "epspx", #Earnings Per Share (Basic) - Excluding Extraordinary Items
                                    "gdwl", #Goodwill
                                    #"gleps", #Gain/Loss Basic EPS Effect
                                    "gp", #Gross Profit (Loss)
                                    #"hedgegl", #Gain/Loss on Ineffective Hedges
                                    "icapt", #Invested Capital - Total
                                    "ivst", #Short-Term Investments - Total
                                    "lct", #Current Liabilities - Total
                                    "lse", #Liabilities and Stockholders Equity - Total
                                    "lt", #Liabilities - Total
                                    "mibt", #Noncontrolling Interests - Total - Balance Sheet
                                    "mkvalt", #Market Value - Total - Fiscal
                                    "ni", #Net Income (Loss)
                                    "rect", #Receivables - Total
                                    "revt", #Revenue Total
                                    "re", #Retained Earnings
                                    #"seteps", #Settlement (Litigation/Insurance) Basic EPS Effect
                                    "spce", #S&P Core Earnings
                                    "spceeps", #S&P Core Earnings EPS Basic
                                    "tfva", #Total Fair Value Assets
                                    "tfvl", #Total Fair Value Liabilities
                                    "txt", #Income Taxes - Total
                                    "txtubsettle", #Settlements with Tax Authorities  
                                    "wcap", #Working Capital (Balance Sheet)   
                                    "xopr" #Operating Expenses - Total
))

DSFu <- DSFund
DSFu <- filter(DSFu, datafmt != "SUMM_STD") 

NACounts <- as.data.frame(colSums(is.na(DSFu)))

# FundYear <- subset(DSFundamentals, select = c("gvkey","fyear","datafmt"))
# FundYear <- filter(FundYear, datafmt != "SUMM_STD") 
# FundSum <- tabyl(FundYear, gvkey, fyear)
# names(FundSum)[names(FundSum) == "2009"] <- "fy2009"
# names(FundSum)[names(FundSum) == "2010"] <- "fy2010"
# names(FundSum)[names(FundSum) == "2011"] <- "fy2011"
# names(FundSum)[names(FundSum) == "2012"] <- "fy2012"
# names(FundSum)[names(FundSum) == "2013"] <- "fy2013"
# FundSum <- FundSum %>% mutate(YearCount = rowSums(.[2:6]))

#Format datadate so it sorts correctly
DSFu$datadate = format(as.Date(DSFu$datadate, format = "%m/%d/%Y"),"%Y/%m/%d")

#Clean up FilingName
DSFu$conm <- toupper(DSFu$conm)
DSFu$conm <- str_trim(DSFu$conm, side = c("both"))
DSFu$conm <- str_replace_all(DSFu$conm, "[[:punct:]]", "")
DSFu$conm <- gsub("[[:space:]]", "_", DSFu$conm)
DSFu$FilingName <- str_replace_all(DSFu$FilingName, "CORPORATION", "CORP")
DSFu$FilingName <- str_replace_all(DSFu$FilingName, "INTERNATIONAL", "INTL")
DSFu$FilingName <- str_replace_all(DSFu$FilingName, "__ADR", "")
names(DSFu)[names(DSFu) == "conm"] <- "FilingName"

#Continuous
#Convert all NA values 0
DSFu$act[is.na(DSFu$act)] <- 0
DSFu$at[is.na(DSFu$at)] <- 0
DSFu$capx[is.na(DSFu$capx)] <- 0
DSFu$ceq[is.na(DSFu$ceq)] <- 0
DSFu$che[is.na(DSFu$che)] <- 0
DSFu$ci[is.na(DSFu$ci)] <- 0
DSFu$cshi[is.na(DSFu$cshi)] <- 0
DSFu$csho[is.na(DSFu$csho)] <- 0
DSFu$dlc[is.na(DSFu$dlc)] <- 0
DSFu$dltt[is.na(DSFu$dltt)] <- 0
DSFu$dv[is.na(DSFu$dv)] <- 0
DSFu$epspx[is.na(DSFu$epspx)] <- 0
DSFu$gdwl[is.na(DSFu$gdwl)] <- 0
#DSFu$gleps[is.na(DSFu$gleps)] <- 0
DSFu$gp[is.na(DSFu$gp)] <- 0
#DSFu$hedgegl[is.na(DSFu$hedgegl)] <- 0
DSFu$icapt[is.na(DSFu$icapt)] <- 0
DSFu$ivst[is.na(DSFu$ivst)] <- 0
DSFu$lct[is.na(DSFu$lct)] <- 0
DSFu$lse[is.na(DSFu$lse)] <- 0
DSFu$lt[is.na(DSFu$lt)] <- 0
DSFu$mibt[is.na(DSFu$mibt)] <- 0
DSFu$ni[is.na(DSFu$ni)] <- 0
DSFu$re[is.na(DSFu$re)] <- 0
DSFu$rect[is.na(DSFu$rect)] <- 0
DSFu$revt[is.na(DSFu$revt)] <- 0
#DSFu$seteps[is.na(DSFu$seteps)] <- 0
DSFu$spce[is.na(DSFu$spce)] <- 0
DSFu$spceeps[is.na(DSFu$spceeps)] <- 0
DSFu$tfva[is.na(DSFu$tfva)] <- 0
DSFu$tfvl[is.na(DSFu$tfvl)] <- 0
DSFu$txt[is.na(DSFu$txt)] <- 0
DSFu$txtubsettle[is.na(DSFu$txtubsettle)] <- 0
DSFu$wcap[is.na(DSFu$wcap)] <- 0
DSFu$xopr[is.na(DSFu$xopr)] <- 0
DSFu$mkvalt[is.na(DSFu$mkvalt)] <- 0

DSFu_Conts <- DSFu
DSFu_Conts <- DSFu %>% group_by(gvkey) %>%
  summarize(
    act = mean(act),
    at = mean(at),
    capx = mean(capx),
    ceq = mean(ceq),
    che = mean(che),
    ci = mean(ci),
    cshi = mean(cshi),
    csho = mean(csho),
    dlc = mean(dlc),
    dltt = mean(dltt),
    dv = mean(dv),
    epspx = mean(epspx),
    gdwl = mean(gdwl),
    #gleps = mean(gleps),
    gp = mean(gp),
    #hedgegl = mean(hedgegl),
    icapt = mean(icapt),
    ivst = mean(ivst),
    lct = mean(lct),
    lse = mean(lse),
    lt = mean(lt),
    mibt = mean(mibt),
    ni = mean(ni),
    re = mean(re),
    rect = mean(rect),
    revt = mean(revt),
    #seteps = mean(seteps),
    spce = mean(spce),
    spceeps = mean(spceeps),
    tfva = mean(tfva),
    tfvl = mean(tfvl),
    txt = mean(txt),
    txtubsettle = mean(txtubsettle),
    wcap = mean(wcap),
    xopr = mean(xopr),
    mkvalt = mean(mkvalt)
  )

#Categorical
#Select the categorical, key, and filter variables from DSFu and merge records down to single row
#Eventually you will join this data frame with the averaged continuous data frame.
DSFu_Cats <- subset(DSFu,select = c("gvkey",
                                    "FilingName",
                                    "datadate",
                                    "datafmt",
                                    "tic",
                                    "ggroup",	#GIC Groups
                                    "gind",	#GIC Industries
                                    "gsector", #GIC Sectors
                                    "gsubind", #GIC Sub-Industries
                                    "sic", #Standard Industry Classification Code
                                    "idbflag", #International, Domestic, Both Indicator
                                    "loc", #Current ISO Country Code - Headquarters
                                    "state", #State/Province
                                    "costat",	#Active/Inactive Status Marker
                                    "acctstd", #Accounting Standard cat
                                    "auop", #Auditor Opinion
                                    "exchg", #Stock Exchange Code
                                    "dlrsn" #Research Co Reason for Deletion
))

#Take the categorical designations associated with the max date available
DSFu_Cats <- DSFu_Cats %>% group_by(gvkey) %>%
  filter(datadate == max(datadate))

#An assignment of three means 'No Opinion' or NA for us.
DSFu_Cats$auop[is.na(DSFu_Cats$auop)] <- 3

DSFu <- merge(DSFu_Cats, DSFu_Conts, by="gvkey")
#colnames(DSFu)[colSums(is.na(DSFu)) > 0]

rm(DSFu_Conts, DSFu_Cats, NACounts)

#DONE FUNDAMENTALS 20210309

##============================================================================

#Read in the Securities data
DSecurities <- fread("D:/Merrimack/Capstone Project/Data/Securities_DS.csv", header=TRUE)
DSecurities <- as.data.frame(DSecurities)
DSecurities <- filter(DSecurities, gsector == "25")
DSec <- subset(DSecurities, select = c("gvkey",	#Global Company Key
                                       "datadate",	#Data Date - Security Monthly
                                       "tic",	#Ticker Symbol
                                       "conm",	#Company Name
                                       "dvrate",	#Dividend Rate - Monthly
                                       "cshtrm",	#Trading Volume - Monthly
                                       "prccm",	#Price - Close - Monthly
                                       "prchm",	#Price - High - Monthly
                                       "prclm",	#Price - Low - Monthly
                                       "trt1m"	#Monthly Total Return
))

#Check how many unique company codes there are
#length(unique(DSecurities$gvkey))

#Add a row index column 
DSec$RowNum <- as.integer(rownames(DSec))

#Format datadate so it sorts correctly
DSec$datadate = format(as.Date(DSec$datadate, format = "%m/%d/%Y"),"%Y/%m/%d")

#Clean up FilingName
DSec$conm <- toupper(DSec$conm)
DSec$conm <- str_trim(DSec$conm, side = c("both"))
DSec$conm <- str_replace_all(DSec$conm, "[[:punct:]]", "")
DSec$conm <- gsub("[[:space:]]", "_", DSec$conm)
names(DSec)[names(DSec) == "conm"] <- "FilingName"

#Clean up Companies with multiple tickers
#Count how many rows per ticker per company and 
#Take the ticker with the largest row count
TickCount <- subset(DSec, select = c("gvkey","FilingName","tic"))
TickCount <- TickCount %>% 
  group_by(gvkey, FilingName, tic) %>% 
  summarise(count=n())

#Get the maximum row count from the previous step and apply it to all gvkeys
TickCount <- TickCount %>% 
  group_by(gvkey) %>% 
  mutate(max_tic = max(count))

#Check to see if a ticker count is the same as the max count per gvkey
#if a ticker count matches the max count, assign a 1
TickCount <- TickCount %>% 
  group_by(gvkey) %>% 
  mutate(is_max_tic = case_when(count == max_tic ~ "1"))

#Filter out rows that aren't labeled with a 1
TickCount <- TickCount %>% 
  filter(is_max_tic == "1") 

#Add a row index column 
TickCount$RowNum <- as.integer(rownames(TickCount))

#This removes rows where a gvkey has two different tickers with an equal row count
#it arbitrarily selects the tic with the largest row number.
TickCount <- TickCount %>% group_by(gvkey) %>%
  filter(RowNum == max(RowNum))

DSecF <- merge(DSec, TickCount, by= c("gvkey","tic"),  all.x=T)

DSecF <- DSecF %>% filter(!is.na(FilingName.y))
DSec <- DSecF
rm(DSecF)

DSec <- DSec %>% group_by(gvkey) %>%
  summarize(
    dvrate = mean(dvrate),
    cshtrm = mean(cshtrm),
    prccm = mean(prccm),
    prchm = mean(prchm),
    prclm = mean(prclm),
    trt1m = mean(trt1m))

#DONE SECURITIES 20210309

##============================================================================

#Read in the Stocks data
DStocks <- fread("D:/Merrimack/Capstone Project/Data/Stocks_DS.csv", header=TRUE)
DStocks <- as.data.frame(DStocks)
DStocks <- filter(DStocks, gsector == "25")
#NACounts <- as.data.frame(colSums(is.na(DStocks)))
DSt <- subset(DStocks, select = c("gvkey",	#Global Company Key
                          "datadate",	#Data Date - Dividends
                          "tic", #Ticker Symbol
                          "conm", #Filing Name
                          #"div",	#Dividends per Share - Ex Date - Daily (Issue) - Daily - too many NAs
                          #"divd",	#Cash Dividends - Daily - too many NAs
                          #"adrrc",	#ADR Ratio - Daily - too many NAs
                          "cshoc",	#Shares Outstanding
                          "cshtrd",	#Trading Volume - Daily
                          "dvi",	#Indicated Annual Dividend - Current
                          "prccd",	#Price - Close - Daily
                          "prchd",	#Price - High - Daily
                          "prcld",	#Price - Low - Daily
                          "prcod"	#Price - Open - Daily
                          #"secstat",	#Security Status Marker WAY TOO MUCH TROUBLE
                          #"dlrsn"	#Research Co Reason for Deletion
                          # "idbflag" #International, Domestic, Both Indicator
))

length(unique(DSt$gvkey))
min(DSt$datadate)
max(DSt$datadate)


#Add a row index column 
DSt$RowNum <- as.integer(rownames(DSt))

#Format datadate so it sorts correctly
DSt$datadate = format(as.Date(DSt$datadate, format = "%m/%d/%Y"),"%Y/%m/%d")

#Clean up FilingName
DSt$conm <- toupper(DSt$conm)
DSt$conm <- str_trim(DSt$conm, side = c("both"))
DSt$conm <- str_replace_all(DSt$conm, "[[:punct:]]", "")
DSt$conm <- gsub("[[:space:]]", "_", DSt$conm)
names(DSt)[names(DSt) == "conm"] <- "FilingName"

#Clean up Companies with multiple tickers
#Count how many rows per ticker per company and 
#Take the ticker with the largest row count
TickCount <- DSt %>% select("gvkey","FilingName","tic")
TickCount <- TickCount %>% 
  group_by(gvkey, FilingName, tic) %>% 
  summarise(count=n())

#Get the maximum row count from the previous step and apply it to all gvkeys
TickCount <- TickCount %>% 
  group_by(gvkey) %>% 
  mutate(max_tic = max(count))

#Check to see if a ticker count is the same as the max count per gvkey
#if a ticker count matches the max count, assign a 1
TickCount <- TickCount %>% 
  group_by(gvkey) %>% 
  mutate(is_max_tic = case_when(count == max_tic ~ "1"))

#Filter out rows that aren't labeled with a 1
TickCount <- TickCount %>% 
  filter(is_max_tic == "1") 

#Add a row index column 
TickCount$RowNum <- as.integer(rownames(TickCount))

#This removes rows where a gvkey has two different tickers with an equal row count
#it arbitrarily selects the tic with the largest row number.
TickCount <- TickCount %>% group_by(gvkey) %>%
  filter(RowNum == max(RowNum))

DStF <- merge(DSt, TickCount, by= c("gvkey","tic"),  all.x=T)

DStF <- DStF %>% filter(!is.na(FilingName.y))
DStF_drops <- c("FilingName.y","RowNum.x","count","max_tic","is_max_tic","RowNum.y")
DStF <- DStF[ , !(names(DStF) %in% DStF_drops)]
DSt <- DStF
rm(DStF, DStF_drops)

#Continuous
#Convert all NA values 0
DSt$cshoc[is.na(DSt$cshoc)] <- 0
DSt$cshtrd[is.na(DSt$cshtrd)] <- 0
DSt$dvi[is.na(DSt$dvi)] <-  0
DSt$prccd[is.na(DSt$prccd)] <- 0
DSt$prchd[is.na(DSt$prchd)] <- 0
DSt$prcld[is.na(DSt$prcld)] <- 0
DSt$prcod[is.na(DSt$prcod)] <- 0

#Taryn - you validated that this subset is correct. 61173 obs is right.
last30 <- DSt %>% group_by(gvkey) %>% top_n(30,datadate)

last30 <- last30 %>% 
  group_by(gvkey) %>% 
  mutate(avg30_cshoc = mean(cshoc))

last30 <- last30 %>% 
  group_by(gvkey) %>% 
  mutate(avg30_cshtrd = mean(cshtrd))

last30 <- last30 %>% 
  group_by(gvkey) %>% 
  mutate(avg30_dvi = mean(dvi))

last30 <- last30 %>% 
  group_by(gvkey) %>% 
  mutate(avg30_prccd = mean(prccd))

last30 <- last30 %>% 
  group_by(gvkey) %>% 
  mutate(avg30_prchd = mean(prchd))

last30 <- last30 %>% 
  group_by(gvkey) %>% 
  mutate(avg30_prcld = mean(prcld))

last30 <- last30 %>% 
  group_by(gvkey) %>% 
  mutate(avg30_prcod = mean(prcod))

last30_drops <- c("datadate","cshoc","cshtrd","dvi","prccd","prchd","prcld","prcod",'datamonth',"tic","FilingName.x","secstat")
last30 <- last30[ , !(names(last30) %in% last30_drops)]
last30 <- unique(last30)

length(unique(DStocks$gvkey))
length(unique(DSt$gvkey))

DSt <- as.data.frame(last30)

#DONE STOCKS 20210217

##============================================================================

#Read in the Ratings File
DRatings <- fread("D:/Merrimack/Capstone Project/Data/Ratings_DS.csv", header=TRUE)
DRatings <- as.data.frame(DRatings)
DRatings <- filter(DRatings, gsector == "25")
DRatings_drops <- c("spsdrm",
                    #"datadate",
                    "city",
                    "conml",
                    "ggroup",
                    "gind",
                    "gsector",
                    "gsubind",
                    "naics",
                    "sic",
                    "state",
                    "conm",
                    "tic",
                    "idbflag",
                    "loc")

DRat <- DRatings[ , !(names(DRatings) %in% DRatings_drops)]

#Add a row index column 
DRat$RowNum <- as.integer(rownames(DRat))

min(DRatings$datadate)
max(DRatings$datadate)

#Get a count of unique values
length(unique(DRatings$gvkey))

#Format datadate so it sorts correctly
DRat$datadate = format(as.Date(DRat$datadate, format = "%m/%d/%Y"),"%Y/%m/%d")

#Convert the S&P Domestic Long Term Issuer Credit Rating to numbers
DRat <- mutate(DRat, splticrm = case_when(
  splticrm ==	"AA"	~	21,
  splticrm ==	"AA-"	~	20,
  splticrm ==	"A+"	~	19,
  splticrm ==	"A"	~	18,
  splticrm ==	"A-"	~	17,
  splticrm ==	"BBB+"	~	16,
  splticrm ==	"BBB-"	~	15,
  splticrm ==	"BBB"	~	14,
  splticrm ==	"BB+"	~	13,
  splticrm ==	"BB"	~	12,
  splticrm ==	"BB-"	~	11,
  splticrm ==	"B+"	~	10,
  splticrm ==	"B"	~	9,
  splticrm ==	"B-"	~	8,
  splticrm ==	"CCC+"	~	7,
  splticrm ==	"CCC"	~	6,
  splticrm ==	"CCC-"	~	5,
  splticrm ==	"CC"	~	4,
  splticrm ==	"D"	~	3,
  splticrm ==	"SD"	~	2,
  splticrm ==	"NA"	~	1))
#Convert all NA values to 1
DRat$splticrm[is.na(DRat$splticrm)] <-1

#Convert the S&P Domestic Short Term Issuer Credit Rating to numbers
DRat <- mutate(DRat, spsticrm = case_when(
  spsticrm ==	"A-1"	~	9,
  spsticrm ==	"A-1+"	~	8,
  spsticrm ==	"A-2"	~	7,
  spsticrm ==	"A-3"	~	6,
  spsticrm ==	"B-1"	~	5,
  spsticrm ==	"B"	~	4,
  spsticrm ==	"D"	~	3,
  spsticrm ==	"C"	~	2,
  spsticrm ==	"NA"	~	1))
#Convert all NA values 1
DRat$spsticrm[is.na(DRat$spsticrm)] <-1

#Convert the S&P Quality Ranking - Current to numbers
DRat <- mutate(DRat, spcsrc = case_when(
  spcsrc ==	"A+"	~	10,
  spcsrc ==	"A"	~	9,
  spcsrc ==	"A-"	~	8,
  spcsrc ==	"B+"	~	7,
  spcsrc ==	"B"	~	6,
  spcsrc ==	"B-"	~	5,
  spcsrc ==	"C"	~	4,
  spcsrc ==	"D"	~	3,
  spcsrc ==	"LIQ"	~	2,
  spcsrc ==	"NA"	~	1))
#Convert all NA values 1
DRat$spcsrc[is.na(DRat$spcsrc)] <-1

#S&P Industry Sector Code
#(unique(DRat$spcindcd))
#Convert all NA values 999
DRat$spcindcd[is.na(DRat$spcindcd)] <- 855

#S&P Economic Sector Code
#unique(DRat$spcseccd)
#Convert all NA values 999
DRat$spcseccd[is.na(DRat$spcseccd)] <-999

DRat <- DRat %>% group_by(gvkey) %>%
  filter(datadate == max(datadate)) 

#Get a count of unique values
length(unique(DRat$gvkey))

DRat_drops <- c("datadate","RowNum")

DRat <- DRat[ , !(names(DRat) %in% DRat_drops)]
DRat <- as.data.frame(DRat)
#DONE - RATINGS

##============================================================================

#Read in the Settlement data
DSCAFS <- fread("D:/Merrimack/Capstone Project/Data/SCA Filings and Settlements.csv", header=TRUE)
DSCAFS <- as.data.frame(DSCAFS)

#Add a row index column 
DSCAFS$RowNum <- as.integer(rownames(DSCAFS))
DSFS <- DSCAFS

# length(unique(DSCAFS$Ticker))
# length(unique(DSCAFS$FilingName))
# min(DSCAFS$FilingYear)
# max(DSCAFS$FilingYear)

#Convert the S&P ratings to numbers then convert field to numeric
DSFS$SettlementAmount <- gsub("[^[:alnum:][:blank:].]","" , DSFS$SettlementAmount ,ignore.case = TRUE)
DSFS$SettlementAmount = as.numeric(DSFS$SettlementAmount)
#Convert all NA values 0
DSFS$SettlementAmount[is.na(DSFS$SettlementAmount)] <-0

DSFSm <- DSFS

#Filing Names cleaned the same way in DSFu dataframe
DSFSm$FilingName <- toupper(DSFSm$FilingName)
DSFSm$FilingName <- str_trim(DSFSm$FilingName, side = c("both"))
DSFSm$FilingName <- str_replace_all(DSFSm$FilingName, "[[:punct:]]", "")
DSFSm$FilingName <- gsub("[[:space:]]", "_", DSFSm$FilingName)
DSFSm$FilingName <- str_replace_all(DSFSm$FilingName, "CORPORATION", "CORP")
DSFSm$FilingName <- str_replace_all(DSFSm$FilingName, "INTERNATIONAL", "INTL")
DSFSm$FilingName <- str_replace_all(DSFSm$FilingName, "__ADR", "")
names(DSFSm)[names(DSFSm) == "Ticker"] <- "tic"


#This didn't end up being useful so removing 202103008
#Count how many settlement rows per company and add back to primary dataframe
# SC <- as.data.frame(table(DSFSm$FilingName))
# names(SC)[names(SC) == "Var1"] <- "FilingName"
# names(SC)[names(SC) == "Freq"] <- "zSttlCount"
# DSFSm <- merge(DSFSm, SC, by = "FilingName")        
# 

SM <- DSFSm %>% group_by(FilingName) %>% summarize(SettleAmount = sum(SettlementAmount))
DSFSm <- merge(DSFSm, SM, by = "FilingName") 

length(unique(DSFSm$FilingName))
length(unique(DSFSm$Ticker))

DSFSm <- subset( DSFSm, select = -c(Exchange,FilingYear, SettlementAmount, Dismissed, RowNum))

DSFSm <- DSFSm %>% group_by(FilingName)%>%filter(SettleAmount == max(SettleAmount))

#Create a new field to indicate whether a settlement occurred or not
DSFSm$SettleYN <-  ifelse(DSFSm$SettleAmount == 0,0,1)

DSFS <- as.data.frame(unique(DSFSm))
DSFS_FN <- subset(DSFS,select = c(FilingName,SettleAmount, SettleYN))
DSFS_TS <- subset(DSFS,select = c(tic,SettleAmount, SettleYN))
rm(DSFSm, SM)


#DONE SETTLEMENTS
rm(DSFinalMerge)
##=== Merge cleansed datasets ===============================================
DSFinalMerge <- merge(DSFu, DSt, by="gvkey",  all.x=T)
DSFinalMerge <- merge(DSFinalMerge, DSec, by="gvkey",  all.x=T)
DSFinalMerge <- merge(DSFinalMerge, DRat, by="gvkey",  all.x=T)
DSFinalMerge <- merge(DSFinalMerge, DSFS_FN, by="FilingName",  all.x=T)
DSFinalMerge <- merge(DSFinalMerge, DSFS_TS, by="tic",  all.x=T)
DSFinalMerge$SettleAmount.x <- ifelse(is.na(DSFinalMerge$SettleAmount.x),DSFinalMerge$SettleAmount.y,DSFinalMerge$SettleAmount.x)
DSFinalMerge$SettleYN.x <- ifelse(is.na(DSFinalMerge$SettleYN.x),DSFinalMerge$SettleYN.y,DSFinalMerge$SettleYN.x)


#Delete some duplicated fields
#DSFinalMerge <- subset(DSFinalMerge,select = -c(datadate.x,datafmt,tic.x,tic.y,datadate.y,FilingName.x))
DSFinalMerge <- subset(DSFinalMerge,select = -c(tic,datadate,datafmt,SettleAmount.y,SettleYN.y))
names(DSFinalMerge)[names(DSFinalMerge) == "SettleAmount.x"] <- "SettleAmount"
names(DSFinalMerge)[names(DSFinalMerge) == "SettleYN.x"] <- "SettleYN"
DSFinalMerge <- unique(DSFinalMerge)

##============================================================================
#Conts
DSFinalMerge$avg30_cshoc[is.na(DSFinalMerge$avg30_cshoc)] <- 0
DSFinalMerge$avg30_cshtrd[is.na(DSFinalMerge$avg30_cshtrd)] <- 0
DSFinalMerge$avg30_dvi[is.na(DSFinalMerge$avg30_dvi)] <- 0
DSFinalMerge$avg30_prccd[is.na(DSFinalMerge$avg30_prccd)] <- 0
DSFinalMerge$avg30_prchd[is.na(DSFinalMerge$avg30_prchd)] <- 0
DSFinalMerge$avg30_prcld[is.na(DSFinalMerge$avg30_prcld)] <- 0
DSFinalMerge$avg30_prcod[is.na(DSFinalMerge$avg30_prcod)] <- 0
DSFinalMerge$cshtrm[is.na(DSFinalMerge$cshtrm)] <- 0
DSFinalMerge$dvrate[is.na(DSFinalMerge$dvrate)] <- 0
DSFinalMerge$prccm[is.na(DSFinalMerge$prccm)] <- 0
DSFinalMerge$prchm[is.na(DSFinalMerge$prchm)] <- 0
DSFinalMerge$prclm[is.na(DSFinalMerge$prclm)] <- 0
DSFinalMerge$trt1m[is.na(DSFinalMerge$trt1m)] <- 0

#Cats
DSFinalMerge$acctstd[is.na(DSFinalMerge$acctstd)] <- "ZZ"
DSFinalMerge$splticrm[is.na(DSFinalMerge$splticrm)] <-1
DSFinalMerge$spsticrm[is.na(DSFinalMerge$spsticrm)] <-1
DSFinalMerge$spcsrc[is.na(DSFinalMerge$spcsrc)] <-1
DSFinalMerge$spcindcd[is.na(DSFinalMerge$spcindcd)] <- 855
DSFinalMerge$spcseccd[is.na(DSFinalMerge$spcseccd)] <-999
DSFinalMerge$state[is.na(DSFinalMerge$state)] <-"ZZ"
DSFinalMerge$state <- ifelse(DSFinalMerge$state == "","ZZ",DSFinalMerge$state)
DSFinalMerge$idbflag <- ifelse(is.na(DSFinalMerge$idbflag) & DSFinalMerge$loc == "USA","D",DSFinalMerge$idbflag)
DSFinalMerge$idbflag <- ifelse(is.na(DSFinalMerge$idbflag) & DSFinalMerge$loc != "USA","B",DSFinalMerge$idbflag)
DSFinalMerge$dlrsn[is.na(DSFinalMerge$dlrsn)] <- 0

#Response Variables
DSFinalMerge$SettleAmount[is.na(DSFinalMerge$SettleAmount)] <- 0
DSFinalMerge$SettleYN[is.na(DSFinalMerge$SettleYN)] <- 0

rm(DRat, DRatings, DSCAFS, DSec, DSecurities, DSFS, DSFu, DSFund,DSFundamentals, DSt, DStocks, last30, TickCount
   ,DRat_drops, DRatings_drops, DSFinalMerge_drops, last30_drops, DSFS_FN, DSFS_TS)

write.csv(DSFinalMerge,"D:/Merrimack/Capstone Project/Data\\DSFinalMerge.csv", row.names = TRUE)



#Invoke the required library packages
library(data.table)
library(dplyr)
library(tidyr)
library(tidyverse)
library(scales)
library(ISLR)
library(MASS)
library(caret)
library(glmnet)
library(pls)
library(leaps)
library(class)
library(ROCR)

set.seed(324) 
setwd("D:/Merrimack/Capstone Project/R Work - Analysis/")

#DS_QL <- read.csv("D:/Merrimack/Capstone Project/Data/DS_QL.csv")
DS_QL <- read.csv("D:/Merrimack/Capstone Project/Data/DSFinalMerge.csv")
DS_QL <- subset(DS_QL,select = -c(X,FilingName))
#source("D:/Merrimack/Capstone Project/R Work - Data Cleansing/Combine_New_20210304.R")
qual1 <- DS_QL
qual1 <- qual1[,c(67,1:65)]
#These cause trouble down the road so remove them here.
qual1 <- subset(qual1,select = -c(idbflag,gsector,spsticrm,dvrate))

########## ########## FEATURE ENGINEERING ########## ##########

qual1$costat <- ifelse(qual1$costat == "A",0,1)
qual1$loc <- ifelse(qual1$loc == "USA",0,1)

#Convert state to numbers
#unique(qual1$state)
qual1$state <- case_when(
  qual1$state == "AB" ~ 1,
  qual1$state == "AL" ~ 2,
  qual1$state == "AR" ~ 3,
  qual1$state == "AZ" ~ 4,
  qual1$state == "BC" ~ 5,
  qual1$state == "CA" ~ 6,
  qual1$state == "CO" ~ 7,
  qual1$state == "CT" ~ 8,
  qual1$state == "DC" ~ 9,
  qual1$state == "DE" ~ 10,
  qual1$state == "FL" ~ 11,
  qual1$state == "GA" ~ 12,
  qual1$state == "HI" ~ 13,
  qual1$state == "IA" ~ 14,
  qual1$state == "ID" ~ 15,
  qual1$state == "IL" ~ 16,
  qual1$state == "IN" ~ 17,
  qual1$state == "KS" ~ 18,
  qual1$state == "KY" ~ 19,
  qual1$state == "LA" ~ 20,
  qual1$state == "MA" ~ 21,
  qual1$state == "MB" ~ 22,
  qual1$state == "MD" ~ 23,
  qual1$state == "MI" ~ 24,
  qual1$state == "MN" ~ 25,
  qual1$state == "MO" ~ 26,
  qual1$state == "MS" ~ 27,
  qual1$state == "MT" ~ 28,
  qual1$state == "NB" ~ 29,
  qual1$state == "NC" ~ 30,
  qual1$state == "NE" ~ 31,
  qual1$state == "NF" ~ 32,
  qual1$state == "NH" ~ 33,
  qual1$state == "NJ" ~ 34,
  qual1$state == "NM" ~ 35,
  qual1$state == "NS" ~ 36,
  qual1$state == "NV" ~ 37,
  qual1$state == "NY" ~ 38,
  qual1$state == "OH" ~ 39,
  qual1$state == "OK" ~ 40,
  qual1$state == "ON" ~ 41,
  qual1$state == "OR" ~ 42,
  qual1$state == "PA" ~ 43,
  qual1$state == "PR" ~ 44,
  qual1$state == "QC" ~ 45,
  qual1$state == "RI" ~ 46,
  qual1$state == "SC" ~ 47,
  qual1$state == "SD" ~ 48,
  qual1$state == "TN" ~ 49,
  qual1$state == "TX" ~ 50,
  qual1$state == "UT" ~ 51,
  qual1$state == "VA" ~ 52,
  qual1$state == "WA" ~ 53,
  qual1$state == "WI" ~ 54,
  qual1$state == "WV" ~ 55,
  qual1$state == "WY" ~ 56,
  qual1$state == "ZZ" ~ 57)

#Convert acctstd to numbers
#unique(qual1$acctstd)
qual1$acctstd <- case_when(
  qual1$acctstd == "DI" ~ 1,
  qual1$acctstd == "DS" ~ 2,
  qual1$acctstd == "DU" ~ 3,
  qual1$acctstd == "ZZ" ~ 4
)


qual1 <- qual1 %>% mutate(liqratio= act/lct)
qual1 <- qual1 %>% mutate(dtaratio= dltt/at)

#colnames(qual1)[colSums(is.na(qual1)) > 0]
qual1$liqratio <- ifelse(qual1$liqratio == "NaN"|qual1$liqratio == "Inf",0,qual1$liqratio)
qual1$dtaratio <- ifelse(qual1$dtaratio == "NaN"|qual1$dtaratio == "Inf" ,0,qual1$dtaratio)

#Make a dataframe that contains only the target row for this class.
#My gvkey is 25119 for Mohawk Inc
MHK <- qual1 %>% filter(gvkey == 25119)
#later you're going to scale this df

#table(qual1$SettleYN)

#colnames(qual1)[colSums(is.na(qual1)) > 0]

#========= Look for outliers =========#

qual_normchecks <- lm(SettleYN ~., data = qual1)
summary(qual_normchecks)
sm <- summary(qual_normchecks)
MSE <-  mean(sm$residuals^2)
MSE
par(mfrow=c(2, 2))
plot(qual_normchecks, main = "Initial Cleansed Data Set")
par(mfrow=c(1, 2))

#========= DEAL WITH OUTLIERS =========#
#Break the data into two sets. One with settlements and one without
SttlRows <- qual1 %>% filter(SettleYN > 0)
NoSttlRows <- qual1 %>% filter(SettleYN == 0)
#create a df to hold the categorical fields
NoSttlRows_cats <- NoSttlRows[,c(1:13)]

#Remove outliers from data set with no Settlement rows
outliers <- function(x) {
  Q1 <- quantile(x, probs=.1)
  Q3 <- quantile(x, probs=.85)
  iqr = Q3-Q1
  upper_limit = Q3 + (iqr*1.5)
  lower_limit = Q1 - (iqr*1.5)
  x > upper_limit | x < lower_limit
}

remove_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    df <- df[!outliers(df[[col]]),]
  }
  df
}
NoSttlRows <- remove_outliers(NoSttlRows[,c(2,14:64)])
#merge the categorical fields back with the continuous fields that now have outliers removed
qual1 <- merge(NoSttlRows_cats,NoSttlRows,by="gvkey")
#Union back with the rows that have a settlement
qual1 <- rbind(qual1, SttlRows)
rm(NoSttlRows, SttlRows, NoSttlRows_cats)

#colnames(qual1)[colSums(is.na(qual1)) > 0]
#NACounts <- as.data.frame(colSums(is.na(quant1)))

qual_normchecks <- lm(SettleYN ~., data = qual1)
summary(qual_normchecks)
sm <- summary(qual_normchecks)
MSE <-  mean(sm$residuals^2)
MSE
par(mfrow=c(2, 2))
plot(qual_normchecks, main = "Outliers Removed Data Set")

#Make Training and Test sets
QLtraining_ind <- createDataPartition(qual1$SettleYN, p = 0.7, list = FALSE, times = 1) 
QLtrain <- qual1[QLtraining_ind, ]
QLtest <- qual1[-QLtraining_ind, ]
QLtestMHK <- rbind(MHK,QLtest)


QLtrainscl <- scale(QLtrain)
scaleList <- list(scale = attr(QLtrainscl, "scaled:scale"),
                               center = attr(QLtrainscl, "scaled:center"))
QLtrainscl <- as.data.frame(scale(QLtrain))

QLtestscl <- as.data.frame(scale(QLtest,
                               center = apply(QLtrain, 2, mean),
                               scale = apply(QLtrain, 2, sd)
))

QLtestMHKscl <- as.data.frame(scale(QLtestMHK,
                                 center = apply(QLtrain, 2, mean),
                                 scale = apply(QLtrain, 2, sd)
))

QLMHKscl <- as.data.frame(scale(MHK,
                              center = apply(QLtrain, 2, mean),
                              scale = apply(QLtrain, 2, sd)))

trainmean <- mean(as.numeric(QLtrainscl$SettleYN))
trainsd <- sd(as.numeric(QLtrainscl$SettleYN))

#We want to scale the data set but retain the response variable as a 0/1 factor.
#Here we check what the response variable has been scaled to and convert it back to an 0/1 factor.
QLtrainscl$SettleYN <- as.factor(ifelse(QLtrainscl$SettleYN <= 0, 0,1))
QLtestscl$SettleYN <- as.factor(ifelse(QLtestscl$SettleYN <= 0, 0,1))
QLtestMHKscl$SettleYN <- as.factor(ifelse(QLtestMHKscl$SettleYN <= 0, 0,1))
QLMHKscl$SettleYN <- as.factor(ifelse(QLMHKscl$SettleYN <= 0, 0,1))

#GLM FIT
glm.fit <- glm(SettleYN ~ 
                 avg30_cshoc	 +
                 spcsrc +
                 liqratio
               ,data = QLtrainscl
               ,family = binomial)

summary(glm.fit)
probs_glm <- predict(glm.fit, QLtestscl, type = "response")
pred.glm <- rep("0", length(probs_glm))
pred.glm[probs_glm > 0.50] <- "1"
ConfMat.glm <- table(pred.QLTr, QLtestscl$SettleYN)
ConfMat.glm 

CM.GLM <- (32+4)/(32+4+11)
CM.GLM

MSE.GLM <- sum(glm.fit$residuals^2)/glm.fit$df.residual
MSE.GLM

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

#Validate the predict function applies the logit function for glm fits
coef(glm.fit)

glm.respscl <-  -0.6839253 + 
  QLMHKscl$avg30_cshoc*7.6743579 + 
  QLMHKscl$spcsrc*1.1136490 + 
  QLMHKscl$liqratio*0.7960210

glm.respscl 

glm.logit.resp.scl <- logit2prob(glm.respscl )
glm.logit.resp.scl

unspred.glmMHK <- (glm.logit.resp.scl*trainsd)+trainmean
likeEst.glm <- unspred.glmMHK

par(mfrow=c(1, 1))
rocr_pred.glm <- prediction(probs_glm , QLtestscl$SettleYN) 
rocr_roc.glm <- performance(rocr_pred.glm, measure = "tpr", x.measure = "fpr") 
plot(rocr_roc.glm, 
     colorize = TRUE, 
     print.cutoffs.at = seq(0, 1, by = 0.1), 
     text.adj = c(-0.5, 1), 
     lwd = 2) 
abline(a = 0, b = 1)

rocr_auc.glm <- performance(rocr_pred.glm, measure = "auc") 
auc.glm <- rocr_auc.glm @y.values[[1]] 
auc.glm

#I don't know exactly what is happening here, but when I run the model
#against the scaled MHK-only data set I get the same predicted value
#as the unscaled MHK prediction generated by the formula from the test set.
#This seems like a good thing, but I don't know how this works.
spMHK <- predict(glm.fit, QLMHKscl, type = "response")
uspMHK <- predict(glm.fit, MHK, type = "response")
logit2prob(uspMHK)
logit2prob(linr.respscl)

##=========== LDA ===========

lda.fit <- lda(SettleYN ~ 
                 avg30_cshoc	 +
                 spcsrc +
                 liqratio 
               , data = QLtrainscl)
lda.fit
summary(lda.fit)
pred.LDA <- data.frame(predict(lda.fit, QLtestscl))
table(pred.LDA$class, QLtestscl$SettleYN)
print(lda.fit)
plot(lda.fit)
#Compute correct classification rate
CM.LDA = (32+3)/(32+3+12)
CM.LDA

#For LDA we want to look at the posterior probabilities to see what class 
#a given data point has been assigned to. 
#This formula calculates the LD1 value for MHK
unspred.ldaMHK <-  QLMHKscl$avg30_cshoc*0.7465578 + 
  QLMHKscl$spcsrc*0.7036389 + 
  QLMHKscl$liqratio*0.6046063
unspred.ldaMHK
#It comes to 0.9263292

#Now I want to confirm the above LD1 value and also find its posterior probabilities
#AND the class its been assigned to.

#First, run your LDA model against a test set that has the MHK row as the first row
#The MHK row was intentionally put at the top of the data set just to make it easier to find
#We did this around line 189 in the code: QLtestMHK <- rbind(MHK,QLtest)
pred.LDAMHK <- data.frame(predict(lda.fit, QLtestMHKscl))
head(pred.LDAMHK)

pred.LDAMHKs <- data.frame(predict(lda.fit, QLMHKscl))
pred.LDAMHKs <- as.data.frame(pred.LDAMHKs)

likeEst.lda <- pred.LDAMHKs$posterior.1

#The head function returns the first row, which we see has an LD1 that matches lda.respscl
#0.9263292

# head(pred.LDAMHK)
# class posterior.0 posterior.1        LD1
# 1      0   0.5808535  0.41914650  0.9263293
# 4      0   0.9727331  0.02726687 -1.4032722
# 8      0   0.9727331  0.02726687 -1.4032722

#So here we can see the posterior probabilities of the class assignment.
#We see that our MHK row with LD1 = .09263 was assigned to class 0 with a 58% probability.
#From what I can gather this means the model is about 58% sure MHK belongs to class 0,
#or it is 58% sure MHK will *not* be sued.
#This means the model is 41.9% sure MHK *will* be sued.
#So the answer to our likelihood estimation is 41.9% chance of being sued.
#I don't think there is any need to unscale anything here.

par(mfrow=c(1, 1))
rocr_pred.lda <- prediction(pred.LDA$posterior.1, QLtestscl$SettleYN) 
rocr_roc.lda <- performance(rocr_pred.lda, measure = "tpr", x.measure = "fpr") 
plot(rocr_roc.lda, 
     colorize = TRUE, 
     print.cutoffs.at = seq(0, 1, by = 0.1), 
     text.adj = c(-0.5, 1), 
     lwd = 2) 
abline(a = 0, b = 1)

rocr_auc.lda <- performance(rocr_pred.lda, measure = "auc") 
auc.lda <- rocr_auc.lda@y.values[[1]] 
auc.lda

##=========== QDA ===========
#Method differs from LDA but produces consistent results.
qda.fit <- train(SettleYN ~ avg30_cshoc + spcsrc + liqratio
                 , method = "qda", data = QLtrainscl)
pred.QDA <- predict(qda.fit, QLtestscl)
CM.QDA <-confusionMatrix(QLtestscl$SettleYN, predict(qda.fit, QLtestscl))
CM.QDA.acc <- as.data.frame(as.matrix(CM.QDA$overall))
CM.QDA.acc <- head(CM.QDA.acc,1)
CM.QDA <- CM.QDA.acc$V1

#We don't get posterior probabilities here, just an overall classification rate
#produced by the CM. So, I just ran the MHK data throuhg the model and the model
#classified it as a 0 or not sued. Since the CM gives us a correct rate of about
#76.6% I guess we just have to surmise that there is a (1-.7659) = .2341 or 23.4%
#chance that MHK *will* be sued.
#Really not sure if that's the right way to think about this.
pred.QDAMHK <- predict(qda.fit, QLMHKscl)
pred.QDAMHK <- as.data.frame(pred.QDAMHK)
likeEst.qda <- (1-CM.QDA)

##============ KNN =====================
QLtrainscl_knn <- QLtrainscl
QLtestscl_knn <- QLtestscl
QLtestMHKscl_knn <- QLtestMHKscl

QLtrainscl_knn$SettleYN <- as.integer(QLtrainscl_knn$SettleYN)
QLtestscl_knn$SettleYN <- as.integer(QLtestscl_knn$SettleYN)

trainX <- as.matrix(QLtrainscl_knn)
testX <- as.matrix(QLtestscl_knn)

trainscl_SettleYN <- QLtrainscl$SettleYN
testscl_SettleYN <- QLtestscl_knn$SettleYN

knn.fit <- knn(trainX, testX, cl=trainscl_SettleYN, k = 9)
predict(knn.fit, QLtestscl_knn)
table(knn.fit, testscl_SettleYN)
mean(knn.fit == testscl_SettleYN)
#Compute Correct classification rate
CM.KNN = (32+6)/(32+6+9)
CM.KNN
likeEst.knn <- CM.KNN

set.seed(1422)
model.knn <- train(SettleYN ~ .,
                   data = QLtrainscl,
                   method = "knn")
print(model.knn)

pred_knn <- predict(model.knn, QLtestscl)
pred.acc.knn <- sum(pred_knn == QLtestscl$SettleYN)/length(QLtestscl$SettleYN)

#This situation is similar to QDA where we don't get an individual % estimate, we just get
#accuracy measures from the CM. Here, the knn fit has an accuracy rate of 80.9%.
#Since the model sorted MHK into the 0/not sued category, we could say that 
#MHK has a (1-.8085) = .1915 chance of being sued, or 19.15% chance of being sued.
pred.knnMHK <- predict(model.knn, QLMHKscl)
likeEst.knn <- (1-CM.KNN)


##============Model Confusion Matrices Comparison ===========================
Model <- c("Logistic Regression"
           ,"LDA"
           ,"QDA"
           ,"KNN")
CM <-c(CM.GLM
        ,CM.LDA
        ,CM.QDA
        ,CM.KNN)
CMs <- cbind(Model,CM)
CMs

par(mfrow=c(1, 1))
par(mar=c(8, 4, 4, 2) + 0.1)
bp<-barplot(c(CM.GLM
              ,CM.LDA
              ,CM.QDA
              ,CM.KNN),
            col = "lightgreen", names.arg = c("Logistic Regression"
                                             ,"LDA"
                                             ,"QDA"
                                             ,"KNN"),
            main = "Confusion Matrices by Models",las=1)
text(bp, 0, format(round(c(CM.GLM
                           ,CM.LDA
                           ,CM.QDA
                           ,CM.KNN),4),big.mark=",", trim=TRUE),cex=1,pos=3)

##=================Likelihood Estimates Comparison ===================
Model <- c("Logistic Regression"
           ,"LDA"
           ,"QDA"
           ,"KNN")
LE <-round(c(likeEst.glm
       ,likeEst.lda
       ,likeEst.qda
       ,likeEst.knn),4)
LEs <- cbind(Model,LE)
LEs

par(mfrow=c(1, 1))
par(mar=c(8, 4, 4, 2) + 0.1)
bp<-barplot(c(likeEst.glm
              ,likeEst.lda
              ,likeEst.qda
              ,likeEst.knn),
            col = "lightblue", names.arg = c("Logistic Regression"
                                             ,"LDA"
                                             ,"QDA"
                                             ,"KNN"),
            main = "Likelihood Estimate by Model",las=1)
text(bp, 0, format(round(c(likeEst.glm
                           ,likeEst.lda
                           ,likeEst.qda
                           ,likeEst.knn),4),big.mark=",", trim=TRUE),cex=1,pos=3)

auc.glm
auc.lda

plot(rocr_roc.glm, col = "red")
plot(rocr_roc.lda, add = TRUE, col = "blue")
plot(p3, add = TRUE, col = "green")

par(mfrow=c(1, 1))
reset.par()
DSQLratio <- as.data.frame(table(DS_QL$SettleYN))
DSQLhist <- barplot(table(DS_QL$SettleYN)
                 , col="maroon"
                 , ylim=c(0,1050)
                 ,main = "Company Settlement Ratio in Initial Data"
                 ,names.arg = c("No Settlement"
                                ,"Settlement"))
text(DSQLhist,0,c(DSQLratio$Freq),cex=1,pos=3)

qual1ratio<- as.data.frame(table(qual1$SettleYN))
qual1hist <- barplot(table(qual1$SettleYN)
                     , col="darkmagenta"
                     , ylim=c(0,120)
                     ,main = "Company Settlement Ratio in Final Data"
                     ,names.arg = c("No Settlement"
                                    ,"Settlement"))
text(DSQLhist,0,c(qual1ratio$Freq),cex=1,pos=3)

######################################################################################################
#Data Science Capstone Project
#Merrimack College - Winter 2021
#Severity Estimation

#Invoke the required library packages
library(data.table)
library(dplyr)
library(tidyr)
library(tidyverse)
library(scales)
library(ISLR)
library(MASS)
library(caret)
library(glmnet)
library(pls)
library(plotrix)

set.seed(324) 
setwd("D:/Merrimack/Capstone Project/R Work - Analysis/")

DSFinalMerge <- read.csv("D:/Merrimack/Capstone Project/Data/DSFinalMerge_New312.csv")
DSFinalMerge_drops <- c("X","FilingName","SettleYN")
DSFinalMerge <- DSFinalMerge[ , !(names(DSFinalMerge) %in% DSFinalMerge_drops)]
#source("D:/Merrimack/Capstone Project/R Work - Data Cleansing/Combine_New_202010222.R")
quant1 <- DSFinalMerge
quant1 <- quant1[,c(66,1:65)]
quant1_drops <- c("gsector","idbflag")
quant1 <- quant1[ , !(names(quant1) %in% quant1_drops)]

########## ########## FEATURE ENGINEERING ########## ##########

quant1$costat <- ifelse(quant1$costat == "A",0,1)
quant1$loc <- ifelse(quant1$loc == "USA",0,1)

#Convert state to numbers
#unique(quant1$state)
quant1$state <- case_when(
quant1$state == "AB" ~ 1,
quant1$state == "AL" ~ 2,
quant1$state == "AR" ~ 3,
quant1$state == "AZ" ~ 4,
quant1$state == "BC" ~ 5,
quant1$state == "CA" ~ 6,
quant1$state == "CO" ~ 7,
quant1$state == "CT" ~ 8,
quant1$state == "DC" ~ 9,
quant1$state == "DE" ~ 10,
quant1$state == "FL" ~ 11,
quant1$state == "GA" ~ 12,
quant1$state == "HI" ~ 13,
quant1$state == "IA" ~ 14,
quant1$state == "ID" ~ 15,
quant1$state == "IL" ~ 16,
quant1$state == "IN" ~ 17,
quant1$state == "KS" ~ 18,
quant1$state == "KY" ~ 19,
quant1$state == "LA" ~ 20,
quant1$state == "MA" ~ 21,
quant1$state == "MB" ~ 22,
quant1$state == "MD" ~ 23,
quant1$state == "MI" ~ 24,
quant1$state == "MN" ~ 25,
quant1$state == "MO" ~ 26,
quant1$state == "MS" ~ 27,
quant1$state == "MT" ~ 28,
quant1$state == "NB" ~ 29,
quant1$state == "NC" ~ 30,
quant1$state == "NE" ~ 31,
quant1$state == "NF" ~ 32,
quant1$state == "NH" ~ 33,
quant1$state == "NJ" ~ 34,
quant1$state == "NM" ~ 35,
quant1$state == "NS" ~ 36,
quant1$state == "NV" ~ 37,
quant1$state == "NY" ~ 38,
quant1$state == "OH" ~ 39,
quant1$state == "OK" ~ 40,
quant1$state == "ON" ~ 41,
quant1$state == "OR" ~ 42,
quant1$state == "PA" ~ 43,
quant1$state == "PR" ~ 44,
quant1$state == "QC" ~ 45,
quant1$state == "RI" ~ 46,
quant1$state == "SC" ~ 47,
quant1$state == "SD" ~ 48,
quant1$state == "TN" ~ 49,
quant1$state == "TX" ~ 50,
quant1$state == "UT" ~ 51,
quant1$state == "VA" ~ 52,
quant1$state == "WA" ~ 53,
quant1$state == "WI" ~ 54,
quant1$state == "WV" ~ 55,
quant1$state == "WY" ~ 56,
quant1$state == "ZZ" ~ 57)

#Convert acctstd to numbers
#unique(quant1$acctstd)
quant1$acctstd <- case_when(
  quant1$acctstd == "DI" ~ 1,
  quant1$acctstd == "DS" ~ 2,
  quant1$acctstd == "DU" ~ 3,
  quant1$acctstd == "ZZ" ~ 4
)

colnames(quant1)[colSums(is.na(quant1)) > 0]

quant1 <- quant1 %>% mutate(liqratio= act/lct)
quant1$liqratio <- ifelse(quant1$liqratio == "NaN"|quant1$liqratio == "Inf",0,quant1$liqratio)

quant1 <- quant1 %>% mutate(dtaratio= dltt/at)
quant1$dtaratio <- ifelse(quant1$dtaratio == "NaN"|quant1$dtaratio == "Inf" ,0,quant1$dtaratio)

#Make a dataframe that contains only the target row for this class.
#My gvkey is 25119 for Mohawk Inc
MHK <- quant1 %>% filter(gvkey == 25119)
#later you're going to scale this df

#examine the cleansed data set before removing outliers etc
quantLM_all <- lm(SettleAmount ~., data = quant1)
summary(quantLM_all)
sm <- summary(quantLM_all)
MSE <-  mean(sm$residuals^2)
MSE
par(mfrow=c(2, 2))
plot(quantLM_all, main = "Initial Cleansed Data Set")
#par(mfrow=c(1, 2))

#========= DEAL WITH OUTLIERS =========#
#Break the data into two sets. One with settlements and one without
SttlRows <- quant1 %>% filter(SettleAmount > 0)
NoSttlRows <- quant1 %>% filter(SettleAmount == 0)
#NoSttlRows <- sample_n(NoSttlRows, 500, replace = FALSE)

#Remove outliers from data set with no Settlement rows
outliers <- function(x) {
  Q1 <- quantile(x, probs=.15)
  Q3 <- quantile(x, probs=.85)
  iqr = Q3-Q1
  upper_limit = Q3 + (iqr*1.5)
  lower_limit = Q1 - (iqr*1.5)
  x > upper_limit | x < lower_limit
}

remove_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    df <- df[!outliers(df[[col]]),]
  }
  df
}
NoSttlRows <- remove_outliers(NoSttlRows,)
#merge the data back together. 
#There will be significantly fewer rows
quant3 <- rbind(SttlRows, NoSttlRows)
quant1 <- quant3

#Check the new data set for normality issues
quantLM_all <- lm(SettleAmount ~.,data = quant1)
summary(quantLM_all)
sm <- summary(quantLM_all)
MSE <-  mean(sm$residuals^2)
MSE
par(mfrow=c(2, 2))
plot(quantLM_all, main = "Outliers Removed Data Set")

#Create the training and test sets
training_ind <- createDataPartition(quant1$SettleAmount, p = 0.7, list = FALSE, times = 1)
train <- quant1[training_ind, ] 
test <- quant1[-training_ind, ]

#Scale the training and test sets and the MHK df

QTtestscl <- as.data.frame(scale(test,
                          center = apply(train, 2, mean),
                          scale = apply(train, 2, sd)))

QTtrainscl <- as.data.frame(scale(train))

QTMHKscl <- as.data.frame(scale(MHK,
                          center = apply(train, 2, mean),
                          scale = apply(train, 2, sd)))

#Unscale by hand
# scale, with default settings, will calculate the mean and standard deviation of the entire vector, 
# then "scale" each element by those values by subtracting the mean and dividing by the sd. 
# (If you use scale(x, scale=FALSE), it will only subtract the mean but not divide by the std deviation.)

trainmean <- mean(train$SettleAmount)
trainsd <- sd(train$SettleAmount)
#Prove it to yourself
# trainmean
# trainsd
# scaledresponse <- (MHK$SettleAmount-trainmean)/trainsd
# unscaledresponse <- (scaledresponse*trainsd)+trainmean

#Linear Regression with Train/Test
lr_trn <- lm(SettleAmount ~
               dlrsn
             + capx
             + ni
             + mkvalt
             + ivst
                 , data = QTtrainscl)
summary(lr_trn)
sm <- summary(lr_trn)
MSE.lr.trn <-  mean(sm$residuals^2)
MSE.lr.trn

pred.lr.test <- predict(lr_trn, QTtestscl)
MSE.lr.test <- mean((pred.lr.test-QTtestscl$SettleAmount)^2)
MSE.lr.test

pred.lr.MHK <- predict(lr_trn, QTMHKscl)
pred.lr.MHK
(pred.lr.MHK*trainsd)+trainmean
((pred.lr.MHK*trainsd)+trainmean)+1.96*trainsd
((pred.lr.MHK*trainsd)+trainmean)-1.96*trainsd

coef(lr_trn)

RespFormula <- 2.356763E-16 + 
  QTMHKscl$dlrsn*0.16127 + 
  QTMHKscl$capx*-3.909607 + 
  QTMHKscl$ni*-12.91534 + 
  QTMHKscl$mkvalt*1.117674 + 
  QTMHKscl$ivst*17.22299 
RespFormula 

unspred.linrMHK <- (RespFormula*trainsd)+trainmean
CI.linrMHK_high <- unspred.linrMHK+1.96*trainsd
CI.linrMHK_low <- unspred.linrMHK-1.96*trainsd
CI.linrMHK_high
CI.linrMHK_low

##### RIDGE
#Convert the scaled train/test sets to matrices
set.seed(324)
trainscl.mat =model.matrix(SettleAmount ~
                             cshtrm
                           +splticrm
                           +prclm
                           +mkvalt
                           , data=QTtrainscl)
testscl.mat =model.matrix(SettleAmount ~
                            cshtrm
                          +splticrm
                          +prclm
                          +mkvalt
                          , data=QTtestscl)
MHKscl.mat =model.matrix(SettleAmount ~
                           cshtrm
                         +splticrm
                         +prclm
                         +mkvalt
                          , data=QTMHKscl)

#Use cross-validation glmnet to choose the best lambda
grid = 10 ^seq(4, -2, length=100)
mod.ridge =cv.glmnet(trainscl.mat, QTtrainscl[, "SettleAmount"], alpha=0, lambda=grid, thresh=1e-12)
lambda.best = mod.ridge$lambda.min
lambda.best

#Put the lambda estimate into the ridge regression model
ridge.pred =predict(mod.ridge, newx=testscl.mat, s=lambda.best)
MSE.ridge <- mean((QTtestscl[, "SettleAmount"] - ridge.pred)^2)
MSE.ridge

coef(mod.ridge)

ridge.respformula <- -1.494035e-16 + 
  QTMHKscl$cshtrm * 7.533586e-02 + 
  QTMHKscl$splticrm * 7.087897e-02 + 
  QTMHKscl$prclm * 6.359994e-02 + 
  QTMHKscl$mkvalt * 5.789613e-02
ridge.respformula 
#This produces 0.9098228
  
#Now unscale the predicted value and create the confidence interval
unspred.ridgeMHK <- (ridge.respformula*trainsd)+trainmean
CI.ridgeMHK_high <- unspred.ridgeMHK +1.96*trainsd
CI.ridgeMHK_low <- unspred.ridgeMHK -1.96*trainsd
unspred.ridgeMHK
CI.ridgeMHK_high
CI.ridgeMHK_low

##### LASSO
#Convert the scaled train/test sets to matrices
set.seed(324)
trainscl.mat =model.matrix(SettleAmount ~
                             dlrsn
                           + capx
                           + ni
                           + mkvalt
                           , data=QTtrainscl)
testscl.mat =model.matrix(SettleAmount ~
                            dlrsn
                          + capx
                          + ni
                          + mkvalt
                          , data=QTtestscl)
MHKscl.mat =model.matrix(SettleAmount ~
                           dlrsn
                         + capx
                         + ni
                         + mkvalt
                         , data=QTMHKscl)

grid = 10^seq(10, -2, length = 100)
fit.lasso <- glmnet(trainscl.mat, QTtrainscl$SettleAmount
                    , alpha = 1
                    , lambda = grid
                    , thresh = 1e-12)

cv.lasso <- cv.glmnet(trainscl.mat, QTtrainscl$SettleAmount
                      , alpha = 1
                      , lambda = grid
                      , thresh = 1e-12)

bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso

lasso.pred =predict(fit.lasso, s=bestlam.lasso, newx=testscl.mat)
MSE.lasso <- mean((QTtestscl[, "SettleAmount"] - lasso.pred)^2)
MSE.lasso

#Get the lasso model coefficients
lasso.coeffs <- predict(fit.lasso, s=bestlam.lasso, type="coefficients")
lasso.coeffs<- data.frame( predict_names = rownames(lasso.coeffs),
                           coef_vals = matrix(lasso.coeffs))

lasso.coeffs

lasso.respformula <- -1.043737e-16 +
  QTMHKscl$dlrsn*0 +
  QTMHKscl$capx*2.588570e-01 +
  QTMHKscl$ni*0 +
  QTMHKscl$mkvalt*4.205302e-01
lasso.respformula

unspred.lassoMHK <- (lasso.respformula*trainsd)+trainmean
CI.lassoMHK_high <- unspred.lassoMHK+1.96*trainsd
CI.lassoMHK_low <- unspred.lassoMHK-1.96*trainsd
unspred.lassoMHK
CI.lassoMHK_high
CI.lassoMHK_low

#####---Compare All Model MSEs 
##============================================================================
Model <- c("Linear MSE"
           ,"Ridge MSE"
           ,"Lasso MSE")
MSE <-c(MSE.lr.trn
        ,MSE.ridge
        ,MSE.lasso)
MSEs <- cbind(Model,MSE)
MSEs

par(mfrow=c(1, 1))
par(mar=c(8, 4, 4, 2) + 0.1)
bp<-barplot(c(MSE.lr.trn
              ,MSE.ridge
              ,MSE.lasso),
            col = "lightblue", names.arg = c("Linear MSE"
                                             ,"Ridge MSE"
                                             ,"Lasso MSE"),
            main = "MSEs of Severity Estimation Models",las=1)
text(bp, 0, format(round(c(MSE.lr.trn
                    ,MSE.ridge
                    ,MSE.lasso),4),big.mark=",", trim=TRUE),cex=1,pos=3)


##======== Compare All Model MHK Unscaled Prediction Values
##============================================================================
Model <- c("Linear Regression"
           ,"Ridge Regression"
           ,"Lasso Regression")
Pred <-c(unspred.linrMHK
        ,unspred.ridgeMHK
        ,unspred.lassoMHK)
Preds <- as.data.frame(cbind(Model,Pred))
Preds$Pred <- as.numeric(Preds$Pred)

PredRange <- 0:max(Preds$Pred)
pts <- pretty(PredRange)

par(mfrow=c(1, 1))
par(mar=c(8, 8, 4, 2) + 0.1)
bp<-barplot(c(unspred.linrMHK
              ,unspred.ridgeMHK
              ,unspred.lassoMHK),
            col = "lavender", names.arg = c("Linear Regression"
                                            ,"Ridge Regression"
                                            ,"Lasso Regression"),
            main = "Settlement Severity Predictions",las=1,ylab ="",axes = F)
title(ylab="US Dollars", line=5.5)
text(bp, 0, format(round(c(unspred.linrMHK
                    ,unspred.ridgeMHK
                    ,unspred.lassoMHK),0),big.mark=",", trim=TRUE),cex=1,pos=3)
axis(2,at = pts, labels=format(pts, scientific=F, big.mark=","), las = 1)


##======== Compare All Model MHK Confidence Intervals
##============================================================================

upperlimit = c(CI.linrMHK_high
               ,CI.ridgeMHK_high
               ,CI.lassoMHK_high)
lowerlimit = c(CI.linrMHK_low
               ,CI.ridgeMHK_low
               ,CI.lassoMHK_low)

mean = c(unspred.linrMHK
         ,unspred.ridgeMHK
         ,unspred.lassoMHK)

names =  c("Linear Regression"
          ,"Ridge Regression"
          ,"Lasso Regression")

namevec <- as.vector(names)

df = data.frame(cbind(upperlimit,lowerlimit,mean))
CIrange <- pretty(max(df$upperlimit):min(df$lowerlimit))

par(mfrow=c(1, 1))
par(mar=c(6, 6, 1, 2)+2 )
cip<-plot(df$mean
     , ylim = c(-500000,40000000)

     ,main="Settlement Severity Predictions 95% Confidence Intervals"
     ,axes = F
     ,xlab = "Model"
     ,ylab = ""
     )
#require(plotrix)
plotCI(df$mean
             ,y=NULL
             , uiw=df$upperlimit-df$mean
             , liw=df$mean-df$lowerlimit
             #, err="y"
             , pch=20
             , slty=3
             , col="red"
             , scol = "blue"
             #, gap=0.02
             , add=TRUE
       ,axes = F
       )
pts <- pretty(CIrange)
title(ylab="US Dollars", line=5.5)
axis(1, at=1:3, labels=namevec) 
axis(2,at = pts, labels=format(pts,scientific=F,big.mark=",", trim=TRUE), las = 1)
